{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Fine-tuning\n","\n","LLM: \n","- https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32\n","\n","Sent-Transformer:\n","- https://huggingface.co/blog/how-to-train-sentence-transformers\n","- https://huggingface.co/datasets/snli\n"]},{"cell_type":"markdown","metadata":{},"source":["Installations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%capture\n","\n","!pip install openai transformers sentence-transformers"]},{"cell_type":"markdown","metadata":{},"source":["Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","import sqlite3\n","\n","import numpy as np\n","import pandas as pd\n","from scipy import spatial\n","\n","from transformers import pipeline\n","from transformers import AutoTokenizer, AutoModelForTokenClassification\n","\n","from sentence_transformers import SentenceTransformer\n","import openai"]},{"cell_type":"markdown","metadata":{},"source":["Using OpenAI"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# If running on Kaggle, add your OpenAI API key to the secrets\n","from kaggle_secrets import UserSecretsClient\n","openai.api_key = UserSecretsClient().get_secret(\"OPENAI_API_KEY\")\n","\n","# # if running locally, use this instead\n","# import os\n","# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n","\n","# ChatGPT using openAI API\n","def openai_generate(prompt: str, llm_model: str = \"text-davinci-003\"):\n","    res = openai.Completion.create(\n","        model=llm_model,\n","        prompt=prompt,\n","        temperature=0,\n","        max_tokens=1024,\n","    )\n","    \n","    choice = res.choices[0]\n","    if choice.finish_reason != \"stop\":\n","        raise Exception(f\"finish reason: {choice.finish_reason}\")\n","    return choice.text\n","\n","# Get the arguments from the prompt\n","# e.g. Sum up all {statement}s and {fact}s -> [\"statement\", \"fact\"]\n","def get_keys(s: str): \n","    res = re.findall(r\"\\{\\S+?\\}\", s)\n","    res = [re.sub(r\"[\\{\\}]\", '', item) for item in res]\n","    return res"]},{"cell_type":"markdown","metadata":{},"source":["Named Entity Recognition (NER) using BERT Transformers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ner_options = dict(\n","    tokenizer = \"dslim/bert-base-NER\",\n","    model = \"dslim/bert-base-NER\",\n",")\n","\n","ner_tokenizer = AutoTokenizer.from_pretrained(ner_options[\"tokenizer\"])\n","ner_model = AutoModelForTokenClassification.from_pretrained(ner_options[\"model\"])\n","ner = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer)\n","\n","# get important topics / tags of a sentence\n","def topics(sentence, ner = ner):\n","    raw_entities = {}\n","    for token in ner(sentence):\n","        if '#' in token[\"word\"] or token[\"entity\"] == \"O\":\n","            continue\n","            \n","        [b_or_i, entity_type] = token[\"entity\"].split(\"-\")\n","        if entity_type not in raw_entities:\n","            raw_entities[entity_type] = [token]\n","            continue \n","            \n","        if b_or_i == \"B\":\n","            raw_entities[entity_type].append(token)\n","        elif b_or_i == \"I\":\n","            raw_entities[entity_type][-1][\"end\"] = token[\"end\"]\n","                \n","    get_token = lambda token: sentence[token['start']:token['end']]\n","        \n","    entities = set()\n","    for entity_type in raw_entities:\n","        for entity in map(get_token, raw_entities[entity_type]):\n","            entities.add(entity + \" (\" + entity_type +  \")\")\n","        \n","    return entities"]},{"cell_type":"markdown","metadata":{},"source":["Using Sentence Transformers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # load pre-trained similarity model\n","\n","# enc_options = dict(\n","#     model = \"sentence-transformers/stsb-roberta-large\",\n","#     dim = 1024,\n","# )\n","enc_options = dict(\n","    model = \"sentence-transformers/all-MiniLM-L6-v2\",\n","    dim = 384,\n",")\n","\n","enc_model = SentenceTransformer(enc_options[\"model\"])"]},{"cell_type":"markdown","metadata":{},"source":["Vector Database functionality"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class VecDBQuery:\n","    def __init__(self, data):\n","        self.data = data\n","    \n","    def result(self):\n","        return self.data\n","    \n","    def generate(\n","        self, \n","        mapper: str = None,\n","        reducer: str = None,\n","    ):\n","        if not len(self.data):\n","            return self\n","        \n","        mapper_args = get_keys(mapper)\n","        reducer_args = get_keys(reducer)\n","        \n","        for i in range(len(self.data)):\n","            self.data[i].update({\"_additional\": {\"generate\": {\n","                \"singleResult\": None,\n","                \"groupedResult\": None, \n","            }}})\n","        \n","        if mapper and len(mapper_args):\n","            for i, item in enumerate(self.data):\n","                single_prompt = f\"{mapper}\\n\\n\" \n","                for arg in mapper_args:\n","                    single_prompt += f\"{arg}: {item[arg]}\\n\"\n","                single_prompt_res = openai_generate(f\"{single_prompt}\\n\")\n","                self.data[i][\"_additional\"][\"generate\"][\"singleResult\"] = single_prompt_res\n","        \n","        if reducer and len(reducer_args):\n","            grouped_prompt = f\"{reducer}\\n\\n\"\n","            for i, item in enumerate(self.data):\n","                for arg in reducer_args:\n","                    grouped_prompt += f\"{arg}[{i}]: {item[arg]}\\n\"\n","            grouped_prompt_res = openai_generate(f\"{grouped_prompt}\\n\")\n","            self.data[0][\"_additional\"][\"generate\"][\"groupedResult\"] = grouped_prompt_res\n","\n","        return self\n","\n","class VecDB:\n","    def __init__(\n","        self, \n","        conn: sqlite3.Cursor,\n","        class_name: str,\n","        keys: list[str],\n","        vectorizer: dict[str, any],\n","        maxchar: int = 1024,\n","    ):\n","        self.conn = conn\n","        self.class_name = class_name\n","        self.keys = list(sorted(keys))\n","        self.indexed_keys = [\"row_num\", *self.keys]\n","        self.maxchar = maxchar\n","        \n","        # create a table with given attributes\n","        # all of which are string with specified max. length\n","        columns = [f'{k} nvarchar({maxchar})' for k in self.keys]\n","        self.conn.execute(f\"CREATE TABLE {class_name} ({' ,'.join(['row_num integer', *columns])})\")\n","\n","        # assign vector options\n","        assert vectorizer is not None, \"vectorizer must be specified\"\n","        \n","        self.vectorizer_fn = vectorizer[\"encoder\"]\n","        self.vectorized_key = vectorizer[\"key\"]\n","        self.vectorizer_dim = vectorizer[\"dim\"]\n","        \n","        # create a vector database\n","        vec_cols = [f'vec{i} float' for i in range(self.vectorizer_dim)]\n","        self.conn.execute(f\"CREATE TABLE vectors ({' ,'.join(vec_cols)})\")\n","    \n","    def insert_data(\n","        self,\n","        data: list[dict[str, any]],\n","    ):     \n","        \n","        # add placeholders for adding values\n","        # then add each row of data\n","        insert_query = f\"INSERT INTO {self.class_name} ({', '.join(self.indexed_keys)}) VALUES ({', '.join(['?']*len(self.indexed_keys))})\"\n","        curr_i = self.conn.execute(f\"SELECT COUNT(row_num) FROM {self.class_name}\").fetchone()\n","        for i, d in enumerate(data):\n","            row_index = curr_i[0] + i\n","            row_values = [row_index] + [d.get(k, '') for k in self.keys]\n","            self.conn.execute(insert_query, row_values)\n","        \n","        # vectorize each data point and add to vector database\n","        new_vectors = self.vectorizer_fn([d[self.vectorized_key] for d in data])\n","        new_vector_tuples = [f\"({', '.join([str(n) for n in vector])})\" for vector in new_vectors]\n","        self.conn.execute(f\"INSERT INTO vectors VALUES {', '.join(new_vector_tuples)}\")\n","        \n","        return self\n","    \n","    # WHERE\n","    # path: if data looks like {\"a\": {\"b\": {\"c\": ...}}}, path is set to [\"a\", \"b\", \"c\"]\n","    # operator: And Or Equal NotEqual GreaterThan GreaterThanEqual LessThan LessThanEqual Like WithinGeoRange IsNull ContainsAny ContainsAll\n","    # valueText, valueInt, valueBoolean etc.\n","    def query_data(\n","        self,\n","        keys: list[str] = None, \n","        near_text: list[str] = None, \n","        where: list[any] = None, \n","        limit: int = None,\n","    ):      \n","        # get all vectors\n","        vectors = np.array(self.conn.execute(\"SELECT * FROM vectors\").fetchall())\n","        \n","        select_query = f\"SELECT {', '.join(['row_num', *sorted(keys)])} FROM {self.class_name}\"\n","        \n","        # get where clauses\n","        where_queries = []\n","        for where_clause in where:\n","            path = where_clause[\"path\"][0]\n","            operator = where_clause[\"operator\"]\n","            value_text = where_clause[\"valueText\"]\n","            if operator == \"ContainsAny\":\n","                patterns = [f\"{path} LIKE '%{val}%'\" for val in value_text]\n","                where_queries.append('WHERE ' + ' OR '.join(patterns))\n","        \n","        # add where clauses and perform select\n","        select_query = \" \".join([select_query, *where_queries])\n","        vals = self.conn.execute(select_query).fetchall()\n","\n","        if not len(vals):\n","            return VecDBQuery(vals)\n","        \n","        # vector update\n","        vectors = vectors[[val[0] for val in vals]]\n","        \n","        min_len = min(limit, len(vals))\n","        \n","        # nearest neighbor search if near_text is specified\n","        if near_text is not None:\n","            near_vector = self.vectorizer_fn(\", \".join(near_text))\n","            searchtree = spatial.KDTree(vectors)\n","                \n","            _, vec_ind = searchtree.query(near_vector, k=min_len)\n","            \n","            # vector update\n","            vectors = vectors[vec_ind]\n","            vals = [vals[i] for i in vec_ind]\n","            \n","            if not len(vals):\n","                return VecDBQuery(vals)\n","        \n","        else:\n","            vals = [vals[i] for i in range(min_len)]\n","            vectors = vectors[:min_len]\n","        \n","        vals = [{k: v for k, v in zip(self.keys, val[1:])} for val in vals]\n","        return VecDBQuery(vals)\n","    \n","    def drop(self):\n","        # drop both tables\n","        self.conn.execute(\"DROP TABLE vectors\")\n","        self.conn.execute(f\"DROP TABLE {self.class_name}\")\n","        return self"]},{"cell_type":"markdown","metadata":{},"source":["Local SQL connection"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T07:49:24.537026Z","iopub.status.busy":"2023-09-27T07:49:24.536588Z","iopub.status.idle":"2023-09-27T07:49:24.544551Z","shell.execute_reply":"2023-09-27T07:49:24.543521Z","shell.execute_reply.started":"2023-09-27T07:49:24.536989Z"},"trusted":true},"outputs":[],"source":["connection = sqlite3.connect(\"sqlite://\")\n","conn = connection.cursor()"]},{"cell_type":"markdown","metadata":{},"source":["Collect and preprocess data"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T07:50:28.039976Z","iopub.status.busy":"2023-09-27T07:50:28.039536Z","iopub.status.idle":"2023-09-27T07:50:28.416102Z","shell.execute_reply":"2023-09-27T07:50:28.414893Z","shell.execute_reply.started":"2023-09-27T07:50:28.039940Z"},"trusted":true},"outputs":[],"source":["class_name = \"statements\"\n","\n","tts = pd.read_csv(\"../input/trump-tweets/trumptweets.csv\",usecols=[\"content\"])\n","sentences = list(tts[\"content\"].values)[:100]\n","\n","data = [\n","    dict(\n","        statement=s, \n","        entities=', '.join(topics(s)),\n","    ) for s in sentences\n","]"]},{"cell_type":"markdown","metadata":{},"source":["Initialize a vector database and insert data into it"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T07:51:07.320342Z","iopub.status.busy":"2023-09-27T07:51:07.319922Z","iopub.status.idle":"2023-09-27T07:51:08.662868Z","shell.execute_reply":"2023-09-27T07:51:08.661635Z","shell.execute_reply.started":"2023-09-27T07:51:07.320307Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dfc86e87d57647e6813ddc70633752d4","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<__main__.VecDB at 0x7cc97615fb20>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["client = VecDB(\n","    conn,\n","    class_name, \n","    keys = [\"statement\", \"entities\"],\n","    vectorizer = dict(\n","        encoder = enc_model.encode,\n","        key = \"statement\",\n","        dim = enc_options[\"dim\"],\n","    ),\n",")\n","\n","client.insert_data(data)"]},{"cell_type":"markdown","metadata":{},"source":["Insert more data into the vector database"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T07:51:30.899917Z","iopub.status.busy":"2023-09-27T07:51:30.899479Z","iopub.status.idle":"2023-09-27T07:51:30.970939Z","shell.execute_reply":"2023-09-27T07:51:30.969859Z","shell.execute_reply.started":"2023-09-27T07:51:30.899885Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d42d2c42acfe4dbf867de86c252c2300","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<__main__.VecDB at 0x7cc97615fb20>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["client.insert_data([\n","    {\n","        \"statement\": \"Donald Trump didn't build any wall in Mexican borders. He built margins.\",\n","        \"entities\": \"Donald Trump (PER)\"\n","    },\n","    {\n","        \"statement\": \"Donald Trump seems to be an inspiring character, but I can assure it's the opposite. He doesn't want you to know that he is betraying the US politics. #AmericanDream\",\n","        \"entities\": \"Donald Trump (PER)\"\n","    }\n","])"]},{"cell_type":"markdown","metadata":{},"source":["Now, feel free to query data by using `query_data` and generate additional output using OpenAI"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T07:52:57.367378Z","iopub.status.busy":"2023-09-27T07:52:57.366989Z","iopub.status.idle":"2023-09-27T07:53:06.629933Z","shell.execute_reply":"2023-09-27T07:53:06.628759Z","shell.execute_reply.started":"2023-09-27T07:52:57.367348Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4ece6b8b31c4e54a56915f08c5b7593","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'entities': 'Donald J. Trump (PER)',\n","  'statement': '\"My persona will never be that of a wallflower - I’d rather build walls than cling to them\" --Donald J. Trump',\n","  '_additional': {'generate': {'singleResult': '\\n\\n1. Building walls is preferable to clinging to them.\\n2. It is not desirable to be a wallflower.',\n","    'groupedResult': '\\nAnswer: []'}}},\n"," {'entities': 'Donald Trump (PER)',\n","  'statement': \"Donald Trump didn't build any wall in Mexican borders. He built margins.\",\n","  '_additional': {'generate': {'singleResult': '\\n1. A wall was not built in Mexican borders.\\n2. Margins were built.',\n","    'groupedResult': None}}},\n"," {'entities': 'Donald Trump (PER), The Late Show (MISC), David Letter (PER)',\n","  'statement': \"-- Watch Donald Trump's recent appearance on The Late Show with David Letterman: http://tinyurl.com/klts6b\",\n","  '_additional': {'generate': {'singleResult': '\\n\\n1. Donald Trump appeared on The Late Show with David Letterman.\\n2. A link to the appearance is http://tinyurl.com/klts6b.',\n","    'groupedResult': None}}},\n"," {'entities': 'Donald Trump (PER), CNN International (ORG), ‘Connect the World’ as ‘Connector of the Day (MISC)',\n","  'statement': 'Donald Trump appearing today on CNN International’s ‘Connect the World’ as ‘Connector of the Day’. Submit questions: http://bit.ly/bPiP7T',\n","  '_additional': {'generate': {'singleResult': \"\\n\\n1. Donald Trump appeared on CNN International's 'Connect the World' today.\\n2. A link to submit questions was provided: http://bit.ly/bPiP7T.\",\n","    'groupedResult': None}}},\n"," {'entities': 'Donald J. Trump (PER)',\n","  'statement': '\"You have to know when to call it quits and when to keep moving forward.\" --Donald J. Trump http://www.trumpthinklikeachampion.com',\n","  '_additional': {'generate': {'singleResult': '\\n\\n1. It is beneficial to know when to end something.\\n2. It is beneficial to know when to continue something.',\n","    'groupedResult': None}}},\n"," {'entities': 'Donald Trump (PER), Facebook (LOC), Donald (MISC)',\n","  'statement': 'Did you know Donald Trump is on Facebook? http://www.facebook.com/DonaldTrump - Become a fan today!',\n","  '_additional': {'generate': {'singleResult': \"\\n\\n1. Donald Trump has a Facebook page.\\n2. The URL for Donald Trump's Facebook page is http://www.facebook.com/DonaldTrump.\",\n","    'groupedResult': None}}},\n"," {'entities': 'Donald J. Trump (PER)',\n","  'statement': '\"We win in our lives by having a champion\\'s view of each moment.\" --Donald J. Trump http://tinyurl.com/pqpfvm',\n","  '_additional': {'generate': {'singleResult': \"\\n1. Winning in life is possible.\\n2. Having a champion's view of each moment is beneficial.\",\n","    'groupedResult': None}}},\n"," {'entities': 'Donald Trump (PER), Think Like A Champion (MISC)',\n","  'statement': 'Listen to an interview with Donald Trump discussing his new book, Think Like A Champion: http://tinyurl.com/qs24vl',\n","  '_additional': {'generate': {'singleResult': '\\n\\n1. Donald Trump has written a book titled Think Like A Champion.\\n2. The book is available for listening in an interview format.\\n3. The interview can be accessed via the URL http://tinyurl.com/qs24vl.',\n","    'groupedResult': None}}},\n"," {'entities': 'Donald J. Trump (PER)',\n","  'statement': '\"Keep it fast, short and direct - whatever it is.\" --Donald J. Trump http://tinyurl.com/pqpfvm',\n","  '_additional': {'generate': {'singleResult': '\\n1. Keep it fast.\\n2. Keep it short.\\n3. Keep it direct.',\n","    'groupedResult': None}}},\n"," {'entities': 'Donald Trump (PER), Neil C (PER), Your World (MISC)',\n","  'statement': 'Hear Donald Trump discuss big gov spending, banks, & taxes on Your World w/Neil Cavuto: http://tinyurl.com/yhnzd7p',\n","  '_additional': {'generate': {'singleResult': '\\n\\n1. Government spending is discussed.\\n2. Banks are discussed.\\n3. Taxes are discussed.',\n","    'groupedResult': None}}}]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["client\\\n","    .query_data(\n","        keys = [\"statement\", \"entities\"],\n","        near_text = [\"wall\", \"politics\"],\n","        where = [dict(\n","            path = [\"entities\"],\n","            operator = \"ContainsAny\",\n","            valueText = [\"Donald J. Trump\", \"Donald Trump\"],\n","        )],\n","        limit = 10,\n","    )\\\n","    .generate(\n","        mapper = \"Extract the facts out of {statement}, also take away the human factor. Results have to be returned in a list of sentences.\",\n","        reducer = \"You are a natural language inference engine. Given many {statement}s, find the conflicting statements (i, j) and return those pairs in a Python list (otherwise return []).\",\n","    )\\\n","    .result()"]},{"cell_type":"markdown","metadata":{},"source":["Drop the both tables"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T07:53:06.709949Z","iopub.status.busy":"2023-09-27T07:53:06.709534Z","iopub.status.idle":"2023-09-27T07:53:06.720012Z","shell.execute_reply":"2023-09-27T07:53:06.718936Z","shell.execute_reply.started":"2023-09-27T07:53:06.709908Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<__main__.VecDB at 0x7cc97615fb20>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["client.drop()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
